import streamlit as st
import cv2
import torch
import numpy as np
import time
from pathlib import Path
import sys
import pandas as pd
from datasets.utils.normalize import normalize
import imgaug.augmenters as iaa
from utils.framer_functions import resize_special

sys.path.insert(0, str(Path(__file__).parent / "src"))
MODEL_PATH = r".\checkpoints\Bukva\best_train_bukva.pth"

st.title("‚úåÔ∏è –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å –†–ñ–Ø (–î–∞–∫—Ç–∏–ª—å) - –†–µ–∂–∏–º –≤–µ–±-–∫–∞–º–µ—Ä—ã")
st.markdown("""
–ù–∞–∂–º–∏—Ç–µ "–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å", –∏ —Å–∏—Å—Ç–µ–º–∞ –∑–∞—Ö–≤–∞—Ç–∏—Ç 40 –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.
""")

with st.expander("‚ÑπÔ∏è –ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç", expanded=False):
    st.markdown("""
    1. **–ù–∞–∂–º–∏—Ç–µ "–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å"** - –∑–∞–ø—É—Å—Ç–∏—Ç—Å—è –æ–±—Ä–∞—Ç–Ω—ã–π –æ—Ç—Å—á–µ—Ç
    2. **–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –∂–µ—Å—Ç** - –≤ —Ç–µ—á–µ–Ω–∏–µ 3 —Å–µ–∫—É–Ω–¥ –ø—Ä–∏–≥–æ—Ç–æ–≤—å—Ç–µ—Å—å –ø–æ–∫–∞–∑–∞—Ç—å –∂–µ—Å—Ç
    3. **–°–∏—Å—Ç–µ–º–∞ –∑–∞—Ö–≤–∞—Ç–∏—Ç 40 –∫–∞–¥—Ä–æ–≤** —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã
    4. **–ö–∞–¥—Ä—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è** (–∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è)
    5. **–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–∞–¥—Ä–æ–≤** –∏ —Ä–∞—Å–ø–æ–∑–Ω–∞—ë—Ç –∂–µ—Å—Ç
    6. **–û—Ç–æ–±—Ä–∞–∂–∞–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç** —Å —É–∫–∞–∑–∞–Ω–∏–µ–º —É—Ä–æ–≤–Ω—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    """)

if 'captured_frames' not in st.session_state:
    st.session_state.captured_frames = []
if 'recognition_result' not in st.session_state:
    st.session_state.recognition_result = None
if 'top3_result' not in st.session_state:
    st.session_state.top3_result = None
if 'is_recording' not in st.session_state:
    st.session_state.is_recording = False
if 'show_countdown' not in st.session_state:
    st.session_state.show_countdown = False

def capture_frames_from_camera(num_frames=40):

    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)
    
    if not cap.isOpened():
        st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–µ–±-–∫–∞–º–µ—Ä—É!")
        return []
    
    #cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
    #cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
    
    #for _ in range(num_frames):
    #    cap.read()
    
    st.info(f"–ó–∞—Ö–≤–∞—Ç—ã–≤–∞–µ–º {num_frames} –∫–∞–¥—Ä–æ–≤ —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã...")
    frames = []
    for i in range(num_frames):
        ret, frame = cap.read()
        if ret:
            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            img = resize_special(img, (360, 270), crop_flag=True)
            frames.append(img)
        else:
            raise ValueError(f"–°–±–æ–π —á—Ç–µ–Ω–∏—è –∫–∞–¥—Ä–∞.")
        
        #time.sleep(0.05)
    
    cap.release()
    
    if frames:
        st.success(f"–ó–∞—Ö–≤–∞—á–µ–Ω–æ {len(frames)} –∫–∞–¥—Ä–æ–≤")
    
    return frames

@st.cache_resource
def load_model():
    try:
        from models.temporal import GestureTransformer
        import json
        with open('src/hyperparameters/Bukva/config.json', 'r') as f:
            config = json.load(f)
        
        backbone = config['network']['backbone']
        n_classes = config['data']['n_classes']
        n_head = config['network']['n_head']
        dropout2d = config['network']['dropout2d']
        dropout1d = config['network']['dropout1d']
        ff_size = config['network']['ff_size']
        n_module = config['network']['n_module']
        pretrained = config['network']['pretrained']

        in_planes = 3
        
        model = GestureTransformer(
            backbone=backbone,
            in_planes=in_planes,
            n_classes=n_classes,
            pretrained=pretrained,
            n_head=n_head,
            dropout_backbone=dropout2d,
            dropout_transformer=dropout1d,
            dff=ff_size,
            n_module=n_module
        )
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        checkpoint = torch.load(MODEL_PATH, map_location=device)
        state_dict = checkpoint['state_dict']
        
        if list(state_dict.keys())[0].startswith('module.'):
            state_dict = {k[7:]: v for k, v in state_dict.items()}
        
        model.load_state_dict(state_dict, strict=False)
        model.eval()
        
        for param in model.parameters():
            param.requires_grad = False
        
        st.success(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        return model
        
    except Exception as e:
        st.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return None

def prepare_frames_for_model(frames):
    clip = list()
    for frame in frames:
        resized = cv2.resize(frame, (224, 224))
        clip.append(resized)
    clip = np.array(clip).transpose(1, 2, 3, 0)
    clip = normalize(clip)
    transforms = iaa.Noop()
    aug_det = transforms.to_deterministic()
    clip = np.array([aug_det.augment_image(clip[..., i]) for i in range(clip.shape[-1])]).transpose(1, 2, 3, 0)
    clip = torch.from_numpy(clip.reshape(clip.shape[0], clip.shape[1], -1).transpose(2, 0, 1))
    clip = clip.float()
    clip = clip.unsqueeze(0)
    return clip


def predict_gesture(model, frames):
    if model is None or len(frames) == 0:
        st.error("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–ª–∏ –Ω–µ—Ç –∫–∞–¥—Ä–æ–≤")
        return "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞", 0.0, []
    
    # --- –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –º–µ—Ç–æ–∫ ---
    label_mapping_path = "./src/datasets/bukva_label_mapping.csv"  # –∏–ª–∏ —É–∫–∞–∂–∏—Ç–µ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    label_df = pd.read_csv(label_mapping_path)
    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –º–µ—Ç–∫–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ label_encoded
    label_df = label_df.sort_values('label_encoded')
    class_names = label_df['text'].tolist()
    
    try:
        input_tensor = prepare_frames_for_model(frames)
        
        with torch.no_grad():
            outputs = model(input_tensor)
            probabilities = torch.nn.functional.softmax(outputs, dim=1)
            top3_conf, top3_idx = torch.topk(probabilities, 3)
            top3_predictions = []
            for i in range(3):
                idx_val = top3_idx[0, i].item()
                conf_val = top3_conf[0, i].item()
                gesture_name = class_names[idx_val] if idx_val < len(class_names) else f"–ö–ª–∞—Å—Å {idx_val}"
                top3_predictions.append((gesture_name, conf_val))

            confidence, predicted_idx = torch.max(probabilities, 1)
            confidence_value = confidence.item()
            predicted_idx_value = predicted_idx.item()
        
        if predicted_idx_value < len(class_names):
            predicted_gesture = class_names[predicted_idx_value]
        else:
            predicted_gesture = f"–ö–ª–∞—Å—Å {predicted_idx_value}"
        
        return predicted_gesture, confidence_value, top3_predictions
        
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏: {e}")
        return "–û—à–∏–±–∫–∞", 0.0, []

st.sidebar.header("üì∑ –ó–∞–ø–∏—Å—å —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã")

st.sidebar.info("""
1. –ù–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É "–ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å"
2. –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ—Å—å –∫ –∂–µ—Å—Ç—É
3. –°–∏—Å—Ç–µ–º–∞ –∑–∞—Ö–≤–∞—Ç–∏—Ç 40 –∫–∞–¥—Ä–æ–≤
4. –ü–æ–ª—É—á–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
""")

if st.sidebar.button("üé¨ –ù–∞—á–∞—Ç—å –∑–∞–ø–∏—Å—å —Å –∫–∞–º–µ—Ä—ã", type="primary", 
                     disabled=st.session_state.is_recording,
                     use_container_width=True):
    st.session_state.is_recording = True
    st.session_state.show_countdown = True
    st.session_state.captured_frames = []
    st.session_state.recognition_result = None
    st.session_state.top3_result = None

if st.session_state.show_countdown:
    countdown_placeholder = st.empty()
    for i in range(3, 0, -1):
        countdown_placeholder.warning(f"‚è≥ –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ—Å—å! –ó–∞–ø–∏—Å—å –Ω–∞—á–Ω–µ—Ç—Å—è —á–µ—Ä–µ–∑ {i}...")
        time.sleep(1)
    
    countdown_placeholder.info("üìπ –ò–¥–µ—Ç –∑–∞–ø–∏—Å—å...")
    st.session_state.show_countdown = False
    
    with st.spinner("–ó–∞—Ö–≤–∞—Ç—ã–≤–∞–µ–º –∫–∞–¥—Ä—ã —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã..."):
        frames = capture_frames_from_camera(40)
        
        if frames:
            st.session_state.captured_frames = frames
            st.session_state.is_recording = False
            
            st.subheader("üì∑ –ó–∞—Ö–≤–∞—á–µ–Ω–Ω—ã–µ –∫–∞–¥—Ä—ã —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã")
            cols = st.columns(4)
            for idx, frame in enumerate(frames):
                with cols[idx % 4]:
                    st.image(frame, caption=f"–ö–∞–¥—Ä {idx+1}", width=150)
            
            if 'model' not in st.session_state:
                with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è..."):
                    st.session_state.model = load_model()
            
            if st.session_state.model:
                with st.spinner("ü§ñ –†–∞—Å–ø–æ–∑–Ω–∞–µ–º –∂–µ—Å—Ç..."):
                    gesture, confidence, top3_list = predict_gesture(st.session_state.model, frames)
                    st.session_state.recognition_result = (gesture, confidence)
                    st.session_state.top3_result = top3_list
                    
                    st.success(f"""
                    ## üéØ –†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
                    ### **–ñ–µ—Å—Ç: {gesture}**
                    –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%}
                    """)
                    
                    if top3_list:
                        st.subheader("üèÜ –¢–æ–ø-3 –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞")
                        cols = st.columns(3)
                        for i, (name, prob) in enumerate(top3_list):
                            with cols[i]:
                                medal_color = ["#FFD700", "#C0C0C0", "#CD7F32"][i]
                                st.markdown(f"<h4 style='text-align: center; color: {medal_color};'>{i+1} –º–µ—Å—Ç–æ</h4>", unsafe_allow_html=True)
                                st.markdown(f"<h3 style='text-align: center;'>{name}</h3>", unsafe_allow_html=True)
                                st.progress(float(prob))
                                st.markdown(f"<p style='text-align: center;'>{prob:.1%}</p>", unsafe_allow_html=True)
                    else:
                        st.info("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã.")
            else:
                st.error("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        else:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞—Ö–≤–∞—Ç–∏—Ç—å –∫–∞–¥—Ä—ã —Å –≤–µ–±-–∫–∞–º–µ—Ä—ã")
            st.session_state.is_recording = False

if st.session_state.recognition_result:
    gesture, confidence = st.session_state.recognition_result
    st.sidebar.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç")
    st.sidebar.metric("–†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π –∂–µ—Å—Ç", gesture)
    st.sidebar.metric("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å", f"{confidence:.1%}")

if 'model' not in st.session_state:
    with st.spinner("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è..."):
        st.session_state.model = load_model()

st.sidebar.divider()
if st.session_state.is_recording:
    st.sidebar.warning("üîÑ –ò–¥–µ—Ç –∑–∞–ø–∏—Å—å...")
else:
    st.sidebar.info("‚úÖ –ì–æ—Ç–æ–≤ –∫ –∑–∞–ø–∏—Å–∏")

st.divider()
st.caption("–°–∏—Å—Ç–µ–º–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –†–ñ–Ø (–î–∞–∫—Ç–∏–ª—å) | –†–µ–∂–∏–º –≤–µ–±-–∫–∞–º–µ—Ä—ã")